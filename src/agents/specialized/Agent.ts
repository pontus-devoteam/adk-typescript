import { BaseAgent } from '../base/BaseAgent';
import { Message, MessageRole } from '../../models/request/LLMRequest';
import { LLMRequest } from '../../models/request/LLMRequest';
import { LLMResponse, ToolCall } from '../../models/response/LLMResponse';
import { RunConfig } from '../../models/config/RunConfig';
import { BaseTool } from '../../tools/base/BaseTool';
import { LLMRegistry } from '../../llm/registry/LLMRegistry';
import { BaseLLM } from '../../llm/BaseLLM';
import { InvocationContext } from '../../models/context/InvocationContext';
import { ToolContext } from '../../models/context/ToolContext';
import { BaseMemoryService } from '../../models/memory/MemoryService';
import { SessionService } from '../../memory/services/SessionService';
import { SearchMemoryOptions } from '../../models/memory/MemoryService';

/**
 * Configuration for Agent
 */
export interface AgentConfig {
  /**
   * Name of the agent
   */
  name: string;
  
  /**
   * Description of the agent
   */
  description: string;
  
  /**
   * The LLM model to use
   */
  model: string;
  
  /**
   * Instructions for the agent
   */
  instructions?: string;
  
  /**
   * Tools available to the agent
   */
  tools?: BaseTool[];
  
  /**
   * Maximum number of tool execution steps
   */
  maxToolExecutionSteps?: number;
  
  /**
   * Memory service for long-term storage and retrieval
   */
  memoryService?: BaseMemoryService;
  
  /**
   * Session service for managing conversations
   */
  sessionService?: SessionService;
  
  /**
   * User ID for the session (required for session persistence)
   */
  userId?: string;
  
  /**
   * Application name (for multi-app environments)
   */
  appName?: string;
  
  /**
   * Whether to automatically augment prompts with relevant memory
   */
  useMemoryAugmentation?: boolean;
  
  /**
   * The maximum number of memory items to include in augmentation
   */
  maxMemoryItems?: number;
  
  /**
   * The minimum relevance score for memory augmentation (0-1)
   */
  memoryRelevanceThreshold?: number;
}

/**
 * Standard Agent implementation that uses an LLM
 */
export class Agent extends BaseAgent {
  /**
   * The LLM model to use
   */
  private model: string;
  
  /**
   * The LLM instance
   */
  private llm: BaseLLM;
  
  /**
   * Instructions for the agent
   */
  private instructions?: string;
  
  /**
   * Tools available to the agent
   */
  private tools: BaseTool[];
  
  /**
   * Maximum number of tool execution steps to prevent infinite loops
   */
  private maxToolExecutionSteps: number;
  
  /**
   * Memory service for long-term storage and retrieval
   */
  private memoryService?: BaseMemoryService;
  
  /**
   * Session service for managing conversations
   */
  private sessionService?: SessionService;
  
  /**
   * User ID for the session
   */
  private userId?: string;
  
  /**
   * Application name
   */
  private appName?: string;
  
  /**
   * Whether to automatically augment prompts with relevant memory
   */
  private useMemoryAugmentation: boolean;
  
  /**
   * The maximum number of memory items to include in augmentation
   */
  private maxMemoryItems: number;
  
  /**
   * The minimum relevance score for memory augmentation (0-1)
   */
  private memoryRelevanceThreshold: number;
  
  /**
   * Constructor for Agent
   */
  constructor(config: AgentConfig) {
    super({
      name: config.name,
      description: config.description
    });
    
    this.model = config.model;
    this.instructions = config.instructions;
    this.tools = config.tools || [];
    this.maxToolExecutionSteps = config.maxToolExecutionSteps || 10;
    this.memoryService = config.memoryService;
    this.sessionService = config.sessionService;
    this.userId = config.userId;
    this.appName = config.appName;
    this.useMemoryAugmentation = config.useMemoryAugmentation ?? false;
    this.maxMemoryItems = config.maxMemoryItems ?? 5;
    this.memoryRelevanceThreshold = config.memoryRelevanceThreshold ?? 0.3;
    
    // Get the LLM instance
    this.llm = LLMRegistry.newLLM(this.model);
  }
  
  /**
   * Finds a tool by name
   */
  private findTool(name: string): BaseTool | undefined {
    return this.tools.find(tool => tool.name === name);
  }
  
  /**
   * Executes a tool call and returns the result
   */
  private async executeTool(
    toolCall: ToolCall,
    context: InvocationContext
  ): Promise<{ name: string, result: any }> {
    const { name, arguments: argsString } = toolCall.function;
    if (process.env.DEBUG === 'true') {
      console.log(`Executing tool: ${name}`);
    }
    
    // Find the tool
    const tool = this.findTool(name);
    if (!tool) {
      console.warn(`Tool '${name}' not found`);
      return {
        name,
        result: `Error: Tool '${name}' not found.`
      };
    }
    
    try {
      // Parse arguments
      const args = JSON.parse(argsString);
      
      // Create a tool execution context
      const toolContext = new ToolContext({
        invocationContext: context,
        parameters: args
      });
      
      toolContext.toolName = name;
      toolContext.toolId = toolCall.id;
      
      // Execute the tool
      const result = await tool.runAsync(args, toolContext);
      if (process.env.DEBUG === 'true') {
        console.log(`Tool ${name} execution complete`);
      }
      
      return {
        name,
        result: typeof result === 'string' ? result : JSON.stringify(result)
      };
    } catch (error) {
      console.error(`Error executing tool ${name}:`, error);
      return {
        name,
        result: `Error executing tool ${name}: ${error instanceof Error ? error.message : String(error)}`
      };
    }
  }
  
  /**
   * Execute multiple tools in parallel
   */
  private async executeTools(
    toolCalls: ToolCall[],
    context: InvocationContext
  ): Promise<{ name: string, result: any, id: string }[]> {
    // Execute all tools in parallel and include the original tool call ID with the result
    const results = await Promise.all(
      toolCalls.map(async toolCall => {
        const result = await this.executeTool(toolCall, context);
        return {
          ...result,
          id: toolCall.id // Include the original tool call ID
        };
      })
    );
    return results;
  }
  
  /**
   * Augments context with relevant memory
   */
  private async augmentWithMemory(context: InvocationContext): Promise<void> {
    // Skip if memory augmentation is disabled or no memory service
    if (!this.useMemoryAugmentation || !context.memoryService) {
      return;
    }
    
    try {
      // Extract query from the last user message
      const lastUserMsg = [...context.messages].reverse().find(msg => msg.role === 'user');
      if (!lastUserMsg || !lastUserMsg.content) {
        return;
      }
      
      // Get the query text
      let query = '';
      if (typeof lastUserMsg.content === 'string') {
        query = lastUserMsg.content;
      } else if (Array.isArray(lastUserMsg.content)) {
        // Extract text from content array
        for (const part of lastUserMsg.content) {
          if (part.type === 'text') {
            query += part.text + ' ';
          }
        }
      } else if (lastUserMsg.content.type === 'text') {
        query = lastUserMsg.content.text;
      }
      
      if (!query.trim()) {
        return;
      }
      
      // Search memory with the query
      const searchOptions: SearchMemoryOptions = {
        threshold: this.memoryRelevanceThreshold,
        limit: this.maxMemoryItems
      };
      
      const memories = await context.searchMemory(query, searchOptions);
      
      // Skip if no relevant memories found
      if (!memories.memories.length) {
        return;
      }
      
      // Generate a summary of relevant memories to add as system message
      const relevantInfo: string[] = [];
      
      for (const memory of memories.memories) {
        const sessionId = memory.sessionId;
        const events = memory.events as Message[];
        
        // Format each memory
        const formattedEvents = events.map(event => {
          const role = event.role === 'user' ? 'User' : 'Assistant';
          let content = '';
          
          if (typeof event.content === 'string') {
            content = event.content;
          } else if (Array.isArray(event.content)) {
            // Extract text from content array
            for (const part of event.content) {
              if (part.type === 'text') {
                content += part.text + ' ';
              }
            }
          } else if (event.content && event.content.type === 'text') {
            content = event.content.text;
          }
          
          return `${role}: ${content}`;
        });
        
        relevantInfo.push(`Session ${sessionId.substring(0, 8)}:\n${formattedEvents.join('\n')}`);
      }
      
      // Add memory information as a system message at the beginning
      if (relevantInfo.length > 0) {
        const memoryMessage: Message = {
          role: 'system',
          content: `Relevant information from previous conversations:\n\n${relevantInfo.join('\n\n')}`
        };
        
        // Insert after existing system messages
        const lastSystemIndex = context.messages.findIndex(m => m.role !== 'system');
        if (lastSystemIndex > 0) {
          context.messages.splice(lastSystemIndex, 0, memoryMessage);
        } else {
          context.messages.unshift(memoryMessage);
        }
      }
    } catch (error) {
      console.error('Error augmenting with memory:', error);
      // Continue without memory augmentation on error
    }
  }
  
  /**
   * Saves the session to memory
   */
  private async saveToMemory(context: InvocationContext): Promise<void> {
    try {
      if (context.sessionService && this.userId) {
        // Ensure we have the user ID set
        context.userId = context.userId || this.userId;
        
        // Save the session
        await context.saveSession();
      }
    } catch (error) {
      console.error('Error saving to memory:', error);
      // Continue without saving to memory on error
    }
  }
  
  /**
   * Generates a unique session ID
   */
  private generateSessionId(): string {
    return `${this.name}_${Date.now()}_${Math.random().toString(36).substring(2, 15)}`;
  }
  
  /**
   * Runs the agent with the given messages and configuration
   */
  async run(options: {
    messages: Message[];
    config?: RunConfig;
    sessionId?: string;
  }): Promise<LLMResponse> {
    // Generate session ID if not provided
    const sessionId = options.sessionId || this.generateSessionId();
    
    const context = new InvocationContext({
      sessionId,
      messages: [...options.messages],
      config: options.config,
      userId: this.userId,
      appName: this.appName,
      memoryService: this.memoryService,
      sessionService: this.sessionService
    });

    try {
      // Add system message with instructions if provided
      if (this.instructions) {
        context.messages.unshift({
          role: 'system',
          content: this.instructions
        });
      }

      // Add tool declarations if any tools are available
      const functions = this.tools.map(tool => tool.getDeclaration());
      
      const responseGenerator = await this.llm.generateContentAsync(new LLMRequest({
        messages: context.messages,
        config: {
          ...options.config,
          functions: functions.length > 0 ? functions : undefined
        }
      }));

      let response: LLMResponse | undefined;
      for await (const chunk of responseGenerator) {
        response = chunk;
      }

      if (!response) {
        throw new Error('No response from LLM');
      }

      let stepCount = 0;
      while (stepCount < this.maxToolExecutionSteps) {
        stepCount++;
        
        if (process.env.DEBUG === 'true') {
          console.log(`\n[Agent] Step ${stepCount}: Thinking...`);
        }
        
        // Request a response from the LLM
        const llmRequest = new LLMRequest({
          messages: context.messages,
          config: {
            ...options.config,
            functions: functions.length > 0 ? functions : undefined
          }
        });
        
        // Use generateContentAsync instead of generateContent
        const responseIterator = this.llm.generateContentAsync(llmRequest);
        let currentResponse: LLMResponse | undefined;
        
        for await (const chunk of responseIterator) {
          currentResponse = chunk;
        }
        
        if (!currentResponse) {
          throw new Error('No response from LLM');
        }
        
        if (currentResponse.tool_calls && currentResponse.tool_calls.length > 0) {
          // The LLM wants to use tools
          if (process.env.DEBUG === 'true') {
            console.log(`[Agent] Executing tools...`);
          }
          
          // Add the assistant message with tool_calls to the conversation first
          context.addMessage({
            role: 'assistant',
            content: currentResponse.content || '',
            tool_calls: currentResponse.tool_calls
          });
          
          // Execute the tools
          const toolResults = await this.executeTools(currentResponse.tool_calls, context);
          
          // Add the results to the context manually
          for (const result of toolResults) {
            context.addMessage({
              role: 'tool',
              tool_call_id: result.id,
              content: result.result
            });
          }
          
          // Continue the conversation loop
          continue;
        } else {
          // This is a final response without tool calls
          if (process.env.DEBUG === 'true') {
            console.log(`[Agent] No tool calls, finishing...`);
          }
          
          // Add the final assistant message to the conversation
          context.addMessage({
            role: 'assistant',
            content: currentResponse.content || ''
          });
          
          // Save to memory if enabled
          await this.saveToMemory(context);
          
          // Return the final response
          return currentResponse;
        }
      }

      return response;
    } catch (error) {
      console.error('Error in agent execution:', error);
      throw error;
    }
  }
  
  /**
   * Runs the agent with streaming support
   */
  async *runStreaming(options: {
    messages: Message[];
    config?: RunConfig;
    sessionId?: string;
  }): AsyncGenerator<LLMResponse> {
    const context = new InvocationContext({
      sessionId: options.sessionId,
      messages: options.messages,
      config: options.config,
      userId: this.userId,
      appName: this.appName,
      memoryService: this.memoryService,
      sessionService: this.sessionService
    });
    
    // Add system instructions if provided
    if (this.instructions) {
      context.messages.unshift({
        role: 'system',
        content: this.instructions
      });
    }
    
    // Augment with memory if enabled
    await this.augmentWithMemory(context);
    
    // Execute agent loop (thinking and tool execution)
    let stepCount = 0;
    let hadToolCalls = false;
    
    while (stepCount < this.maxToolExecutionSteps) {
      if (process.env.DEBUG === 'true') {
        console.log(`\n[Agent] Step ${stepCount + 1}: Thinking...`);
      }
      
      // Get tool declarations
      const toolDeclarations = this.tools
        .map(tool => tool.getDeclaration())
        .filter(declaration => declaration !== null);
      
      // Create the request
      const request = {
        messages: context.messages,
        config: {
          functions: toolDeclarations as any[],
          stream: true
        }
      };
      
      // Stream response from LLM
      const responseGenerator = this.llm.generateContentAsync(request);
      
      // Capture the final response for tool execution
      let finalResponse: LLMResponse | null = null;
      hadToolCalls = false;
      
      // Stream response chunks
      for await (const responseChunk of responseGenerator) {
        // Update current full response
        finalResponse = responseChunk;
        
        // Yield the chunk
        yield responseChunk;
        
        // Check if there are tool calls to execute
        hadToolCalls = 
          Boolean(responseChunk.function_call) || 
          (responseChunk.tool_calls !== undefined && responseChunk.tool_calls.length > 0);
      }
      
      if (!finalResponse) {
        throw new Error("No response received from LLM");
      }
      
      // Add assistant response to conversation history
      context.addMessage({
        role: 'assistant',
        content: finalResponse.content || '',
        function_call: finalResponse.function_call
      });
      
      // If no tool calls, we're done
      if (!hadToolCalls) {
        if (process.env.DEBUG === 'true') {
          console.log(`[Agent] No tool calls, finishing...`);
        }
        break;
      }
      
      if (process.env.DEBUG === 'true') {
        console.log(`[Agent] Executing tools...`);
      }
      stepCount++;
      
      // Execute function_call for backward compatibility
      if (finalResponse.function_call) {
        const toolCall = {
          id: 'function-call',
          function: {
            name: finalResponse.function_call.name,
            arguments: finalResponse.function_call.arguments
          }
        };
        
        // For function_call, we don't modify the assistant message that was already added
        
        const [result] = await this.executeTools([toolCall], context);
        
        // Add function response to conversation history
        context.addMessage({
          role: 'function',
          name: result.name,
          content: JSON.stringify(result.result)
        });
      } 
      // Execute tool_calls
      else if (finalResponse.tool_calls && finalResponse.tool_calls.length > 0) {
        if (process.env.DEBUG === 'true') {
          console.log(`[Agent] Executing ${finalResponse.tool_calls.length} tool(s)...`);
        }
        
        // Replace the assistant message with one that includes tool_calls
        // First, remove the last message (which should be the assistant message we just added)
        context.messages.pop();
        
        // Add the assistant message with tool_calls
        context.addMessage({
          role: 'assistant',
          content: finalResponse.content || '',
          tool_calls: finalResponse.tool_calls
        });
        
        const results = await this.executeTools(finalResponse.tool_calls, context);
        
        // Add tool responses to conversation history
        for (const result of results) {
          context.addMessage({
            role: 'tool',
            tool_call_id: result.id,
            content: result.result
          });
        }
      }
    }
    
    // Save to memory if enabled
    await this.saveToMemory(context);
  }
} 